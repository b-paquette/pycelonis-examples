{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5db316",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ac664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycelonis import get_celonis\n",
    "from pycelonis import pql\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920606d7",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c2b94",
   "metadata": {},
   "source": [
    "#### From DM (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_celonis()\n",
    "\n",
    "# Set DM\n",
    "\n",
    "dm_id = 'input_data_dm_id'\n",
    "dm = c.datamodels.find(dm_id)\n",
    "dm\n",
    "\n",
    "# Set PQL Query of the data to load\n",
    "\n",
    "query = pql.PQL()\n",
    "\n",
    "cols_to_load = [(\"table_1.col_name_1\", \"col_pretty_name_1\")\n",
    "            ,(\"table_2.col_name_2\", \"col_pretty_name_2\")\n",
    "               ]\n",
    "\n",
    "for col,pretty_col in cols_to_load:\n",
    "    query += pql.PQLColumn(col,pretty_col)\n",
    "\n",
    "query += pql.PQLFilter(\"FILTER TBD;\")\n",
    "\n",
    "loaded_df = dm._get_data_frame(query)\n",
    "\n",
    "loaded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b7483",
   "metadata": {},
   "source": [
    "#### From Analysis (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_celonis()\n",
    "\n",
    "workspace_id = \"TBD\"\n",
    "workspace = c.workspaces.find(workspace_id)\n",
    "\n",
    "analysis_name = 'TBD'\n",
    "analysis = workspace.analyses.find(analysis_name, False)\n",
    "component = analysis.draft.components.find(\"TBD\")\n",
    "loaded_df = component.get_data_frame()\n",
    "\n",
    "loaded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b87de",
   "metadata": {},
   "source": [
    "##### Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df.shape\n",
    "\n",
    "# Feature distribution\n",
    "col_name = 'TBD'\n",
    "print('For col ',col_name,' # NaN values are ',loaded_df[col_name].isna().shape[0])\n",
    "print('Value Distribution is ',loaded_df[col_name].value_counts().sort_values(ascending=False))\n",
    "\n",
    "# Target Distribution\n",
    "target_col = 'TBD'\n",
    "print(loaded_df[target_col].isna().shape[0])\n",
    "print(loaded_df[target_col].value_counts().sort_values(ascending=False))\n",
    "100.0*loaded_df[target_col].mean()\n",
    "\n",
    "# Plot Feature vs Target\n",
    "plt.scatter(loaded_df[col_name],loaded_df[target_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12732c8f",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a87689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of loaded df\n",
    "\n",
    "input_df = loaded_df.copy()\n",
    "\n",
    "# Filter rows\n",
    "\n",
    "input_df = input_df[input_df['col_to_filter'] == 'condition to match']\n",
    "input_df = input_df[input_df['date_col_to_filter'] >= dt.date(2021,2,10)] # Date filter\n",
    "input_df = input_df[input_df['col_to_filter'] >= 'min condition']\n",
    "input_df = input_df[input_df['col_to_filter'] <= 'max condition']\n",
    "input_df = input_df[input_df['col_to_filter'].str.contains('string pattern to filter on')] # Filter based on column contains a certain string pattern\n",
    "\n",
    "# Clean values\n",
    "\n",
    "input_df = input_df.dropna() # Remove rows with some empty values\n",
    "input_df = input_df[~input_df['col'].isna()] # Remove rows with empty value in a certain column\n",
    "\n",
    "input_df.loc[input_df['column'].isna()] = 'value_for_na' # Fill empty values of a certain column with a value\n",
    "\n",
    "# Change data types\n",
    "\n",
    "input_df = input_df['column'].astype(int) # Cast column as certain type, such as Int\n",
    "input_df = input_df['date_column'].dt.date() # Cast date column as Date type\n",
    "\n",
    "# Sort rows\n",
    "\n",
    "input_df = input_df.sort_values(by=['column_to_sort_on'],ascending=True)\n",
    "\n",
    "# Set row ID as index\n",
    "\n",
    "input_df['CASE_KEY'] = input_df['pk1'] + input_df['pk2'] + input_df['pk3']\n",
    "input_df = input_df.set_index('CASE_KEY')\n",
    "\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486b4f3",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39578487",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select columns for the model\n",
    "\n",
    "feature_cols = ['col1'\n",
    "                ,'col2']\n",
    "input_df = input_df[feature_cols]\n",
    "\n",
    "## Transform existing columns\n",
    "\n",
    "input_df['column'] = input_df['column']**2 # Square existing column\n",
    "input_df['column'] = np.sqrt(input_df['column']) # SQRT existing column\n",
    "\n",
    "## Create new columns\n",
    "\n",
    "input_df['new_column'] = input_df['column_1'] + input_df['column_2'] # New column with columns 1 and 2\n",
    "input_df['new_column'] = np.where(input_df['column_1'] == 'condition','Value_if_true','Value_if_false') # Create flag based on a column\n",
    "\n",
    "input_df['new_column'] = input_df.groupby('column_to_group_by')['column_to_aggregate'].transform('mean') # Aggregate column, such as Average or Sum\n",
    "input_df['new_column'] = input_df.groupby('column_to_group_by')['column_to_aggregate'].rolling(7).mean().reset_index(0,drop=True) # Rolling average\n",
    "input_df['new_shifted_column'] = input_df.groupby('column_to_group_by')['column_to_shift'].shift(-1) # Shift values\n",
    "\n",
    "## Encode categorical columns\n",
    "\n",
    "input_df = pd.get_dummies(input_df,columns=['col1'],drop_first=True)\n",
    "\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fbf6fc",
   "metadata": {},
   "source": [
    "### Train/Test/Forecast Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Target=y column\n",
    "\n",
    "target_col = 'target_col_name'\n",
    "\n",
    "# Forecast mask\n",
    "\n",
    "forecast_msk = input_df[target_col].isna()\n",
    "\n",
    "# X/y split\n",
    "\n",
    "train_test_df = input_df[~forecast_msk]\n",
    "\n",
    "X_df = train_test_df.drop(columns=[target_col])\n",
    "y_df = train_test_df[target_col]\n",
    "\n",
    "\n",
    "## Train/Test split OPTION 1: Set split w/ mask\n",
    "\n",
    "msk = ~forecast_msk&(input_df['date_column']<dt.date(2021,2,10)) # Date split\n",
    "\n",
    "msk = (input_df['value_column']<='value_for_split') # Value split\n",
    "\n",
    "X_train, X_test, y_train, y_test = X_df[msk],X_df[~msk],y_df[msk],y_df[~msk]\n",
    "\n",
    "\n",
    "## Train/Test split OPTION 2: Random split w/ Scikit-learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849a6b7d",
   "metadata": {},
   "source": [
    "### Train Model (choose between Regression and Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c40f8",
   "metadata": {},
   "source": [
    "#### Regression model (predict a value i.e. # Sales Orders per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTION REGRESSION - Set the XGBoost model\n",
    "# Doc here: https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "xgb_model = xgb_regression = xgb.XGBRegressor(objective='reg:squarederror'\n",
    "                                  \n",
    "                                  # IMPORTANT - max depth of trees\n",
    "                                  ,max_depth=5\n",
    "                                  \n",
    "                                   # IMPORTANT - learning speed of the algoritm, the smaller the more precise but slower e.g. needs higher n_estimators\n",
    "                                  ,learning_rate=0.1\n",
    "                                   \n",
    "                                  # % columns used at each tree split, can be defaulted to 1\n",
    "                                  ,colsamply_bytree=0.8\n",
    "                                \n",
    "                                  # Regularization factor, to avoid overfitting\n",
    "                                  ,alpha=10\n",
    "                                  \n",
    "                                  # IMPORTANT - # trees\n",
    "                                  ,n_estimators=30\n",
    "                                 )\n",
    "\n",
    "## Fit model to train set\n",
    "\n",
    "# standard Option: fit train\n",
    "xgb_model.fit(X_train,y_train,verbose=True)\n",
    "\n",
    "# better Option: fit train and validate on test\n",
    "eval_set = [(X_train, y_train),(X_test, y_test)]\n",
    "xgb_model.fit(X_train, y_train, eval_metric=[\"rmse\"], eval_set=eval_set, verbose=True)\n",
    "\n",
    "xgg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f0c3c",
   "metadata": {},
   "source": [
    "#### Classification model (predict a category i.e. 'Late' or 'On time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION CLASSIFICATION - Set the XGBoost model\n",
    "# Doc here: https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "xgb_model = xgb_classifier = xgb.XGBClassifier(objective='binary:logistic'\n",
    "                                  \n",
    "                                  # IMPORTANT - max depth of trees\n",
    "                                  ,max_depth=8\n",
    "                                  \n",
    "                                   # IMPORTANT - learning speed of the algoritm, the smaller the more precise but slower e.g. needs higher n_estimators\n",
    "                                  ,learning_rate=0.1\n",
    "                                   \n",
    "                                  # % columns used at each tree split, can be defaulted to 1\n",
    "                                  ,colsample_bytree=1.0\n",
    "                                \n",
    "                                  # Regularization factor, to avoid overfitting\n",
    "                                  #,reg_lambda=10\n",
    "                                  \n",
    "                                  # IMPORTANT - # trees\n",
    "                                  ,n_estimators=30\n",
    "                                  \n",
    "                                  # % Subsample of the dataset to consider for Train (if Train set is too large)\n",
    "                                  ,subsample=1.0\n",
    "                                 )\n",
    "\n",
    "## Train model on Train set\n",
    "\n",
    "# standard Option: fit train\n",
    "xgb_model.fit(X_train,y_train,verbose=True)\n",
    "\n",
    "# better Option: fit train and validate on test\n",
    "eval_set = [(X_train, y_train),(X_test, y_test)]\n",
    "xgb_model.fit(X_train, y_train, eval_metric=[\"logloss\"], eval_set=eval_set, verbose=True)\n",
    "\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552f2aa",
   "metadata": {},
   "source": [
    "#### Save model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf57cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Results to csv\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "xgb_model.save_model('xgb_model_'+str(today)+'.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3489a7",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417a8c0",
   "metadata": {},
   "source": [
    "#### Run model on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbda513",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_test has shape... ',X_test.shape)\n",
    "print('Should have as many columns as X_train with shape... ',X_train.shape)\n",
    "\n",
    "## Predict y for Test set\n",
    "pred_y_test = xgb_model.predict(X_test)\n",
    "# Turn into Df\n",
    "pred_test_df = pd.DataFrame(pred_y_test,index=X_test.index,columns=['PREDICTION'])\n",
    "\n",
    "## (Optional) For binary Classification: Predict probabilities too\n",
    "proba_pred_y_test = xgb_model.predict_proba(X_test)[:,1]\n",
    "# Turn into Df\n",
    "proba_pred_test_df = pd.DataFrame(proba_pred_y_test,index=X_test.index,columns=['PROBA_PREDICTION'])\n",
    "\n",
    "\n",
    "## Add columns to Test Predictions Df (such as Case Key or attributes like Country, Material type etc)\n",
    "\n",
    "def add_columns_to_results(results_df,cols_df,col_names):\n",
    "    if col_names is None:\n",
    "        enriched_results = results_df.join(cols_df,how='left')\n",
    "    else:\n",
    "        enriched_results = results_df.join(cols_df[col_names],how='left')\n",
    "    return enriched_results\n",
    "\n",
    "# Add Proba Preds y to df\n",
    "pred_test_df = add_columns_to_results(pred_test_df,proba_pred_test_df,None)\n",
    "\n",
    "# Add True y to df\n",
    "pred_test_df = add_columns_to_results(pred_test_df,y_test,None)\n",
    "\n",
    "# Add input columns to df\n",
    "cols_to_add = list(input_df.columns)\n",
    "cols_to_add.remove(target_col)\n",
    "pred_test_df = add_columns_to_results(pred_test_df,input_df,cols_to_add)\n",
    "\n",
    "## Print Test Predictions Df\n",
    "pred_test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cad3cb",
   "metadata": {},
   "source": [
    "#### Evaluate Model on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option REGRESSION\n",
    "\n",
    "# TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option CLASSIFICATION\n",
    "\n",
    "# Feature importance\n",
    "\n",
    "xgb.plot_importance(xgb_model)\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_y_test).ravel()\n",
    "print('Confusion matrix ',confusion_matrix(y_test, pred_y_test))\n",
    "\n",
    "# Precision = % correct positives = True Positives / (True Positives + False Positives)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, pred_y_test, average='binary')\n",
    "print('Precision: ',precision)\n",
    "\n",
    "# Recall = % detected positives = TruePositives / (TruePositives + FalseNegatives)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall = recall_score(y_test, pred_y_test, average='binary')\n",
    "print('Recall: ',recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ababd",
   "metadata": {},
   "source": [
    "### Forecast Model (Optional, for out-of-sample Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb69176",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict y for Forecast set\n",
    "pred_y_forecast = xgb_model.predict(X_forecast)\n",
    "\n",
    "pred_forecast_df = pd.DataFrame(pred_y_forecast,index=X_forecast,columns=['PREDICTION'])\n",
    "\n",
    "## Combine Forecasts and Test predictions in same Df\n",
    "\n",
    "# Add input columns to Forecast Preds Df\n",
    "print('Adding same input columns as for Test Df: ',cols_to_add)\n",
    "pred_forecast_df = add_columns_to_results(pred_forecast_df,input_df,cols_to_add)\n",
    "# Add Subset column for Test vs Forecast\n",
    "pred_test_df['Subset'] = 'Test'\n",
    "pred_forecast_df['Subset'] = 'Forecast'\n",
    "\n",
    "# Combine in same Df\n",
    "all_preds_df = pd.concat([pred_test_df, pred_forecast_df])\n",
    "\n",
    "# Print Test & Forecast Df\n",
    "all_preds_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f7f96",
   "metadata": {},
   "source": [
    "#### Export Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7ecb2",
   "metadata": {},
   "source": [
    "##### Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Predictions df for Export\n",
    "export_preds_df = all_preds_df.copy()\n",
    "\n",
    "# Rename columns (to match current DM tables field names if it helps)\n",
    "cols_to_rename = {'col_1':'new_col_1','col_2':'new_col_2'}\n",
    "export_preds_df = export_preds_df.rename(columns=cols_to_rename)\n",
    "\n",
    "# Add other columns (for Joining purposes or Analysis drilldowns)\n",
    "\n",
    "cols_to_add_to_export = ['col_1','col_2']\n",
    "export_preds_df = add_columns_to_results(export_preds_df,loaded_df,cols_to_add_to_export)\n",
    "\n",
    "export_preds_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d3a7e",
   "metadata": {},
   "source": [
    "##### Push Results to DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c51761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify export Predictions one last time before push to DM\n",
    "print('Shape of the Export is ',export_preds_df.shape)\n",
    "\n",
    "print('Test and Forecast sets sizes are ',export_preds_df['Subset'].value_counts())\n",
    "\n",
    "print('Test predictions preview is ',export_preds_df[export_preds_df['Subset']=='Test'].head())\n",
    "print('Forecast predictions preview is ',export_preds_df[export_preds_df['Subset']=='Forecast'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26158ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DM where Predictions export should be pushed to\n",
    "export_dm_id = 'export_DM_ID'\n",
    "export_dm = c.datamodels.find(export_dm_id)\n",
    "export_dm\n",
    "\n",
    "# Push Predictions to DM\n",
    "table_name = '_ML_PREDICTIONS'\n",
    "export_dm.push_table(exports_preds_df,,if_exists='replace',reload_datamodel = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b70ce4",
   "metadata": {},
   "source": [
    "#### Push Results to DM, with Historical table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_table_name = f\"_ML_PREDICTIONS_HISTORY\"\n",
    "try:\n",
    "    dm.pool.append_table(\n",
    "        df_or_path=exports_preds_df,\n",
    "        table_name=history_table_name,\n",
    "    )\n",
    "except ValueError: # Table does not exist\n",
    "    print(f\"Create table {history_table_name}\")\n",
    "    dm.pool.create_table(\n",
    "        df_or_path=exports_preds_df,\n",
    "        table_name=history_table_name,\n",
    "        if_exists=\"error\"\n",
    "    )\n",
    "\n",
    "    print(\"Add table to datamodel...\")\n",
    "    dm.add_table_from_pool(history_table_name)\n",
    "    source_table = 'TABLE_TO_JOIN_TO'\n",
    "    target_table = history_table_name\n",
    "    dm.create_foreign_key(source_table,\n",
    "                          target_table,\n",
    "                          [(col.name, col.name) for col in ['join_col1','join_col2']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
